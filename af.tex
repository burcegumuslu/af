\documentclass{article}
\usepackage{graphicx} 
\newtheorem{definition}{Definition}
\newcounter{subdefinition}[definition]
\renewcommand{\thesubdefinition}{\thedefinition.\arabic{subdefinition}}
\newenvironment{subdefinition}{
        \refstepcounter{subdefinition}
        \par\noindent
        \textbf{\indent \upshape Definition \thesubdefinition}%
}{}
\newtheorem{theorem}{Theorem}
\usepackage{tikz}
\usepackage{apacite}
\usepackage[title]{appendix}
\usepackage{amsmath,amssymb, amsfonts, mathtools, cancel}
\usepackage{hyperref}
\usetikzlibrary{shapes,shadows,arrows,positioning}


\title{Algorithmic Fairness: A Necessity Result}
\author{Zeynep Burce Gumuslu}
\date{AI Alignment Seminar, December 2024}

\begin{document}

\maketitle
\section{Introduction}
%IntroductionIntroduction
%IntroductionIntroduction
%IntroductionIntroduction

Predictive algorithms have been playing an increasingly prominent role in decision-making processes that significantly impact the well-being and prospects of individuals and groups. These algorithms are extensively used in fields such as criminal justice, healthcare, banking, employment, and education, where their outputs shape critical decisions ranging from sentencing recommendations to hiring outcomes. While these systems promise efficiency and consistency, concerns about their potential biases and unfairness have garnered substantial attention. These concerns are not merely hypothetical but are substantiated by numerous case studies revealing that real-world algorithmic systems often exhibit biases against somef socially salient groups.

Arising from such concerns, algorithmic fairness is a field of study that seeks to evaluate decision-making procedures based on their fairness, particularly in relation to what are often called \textit{sensitive traits}, such as race, gender, ethnicity, and religion. A central challenge in this field is identifying appropriate criteria for assessing fairness in rule-based procedures. One influential approach involves defining statistical criteria for algorithmic fairness. These criteria are considered to be necessary conditions that the outcomes of an algorithmâ€™s predictions must meet for the algorithm to be deemed fair. Prominent examples of statistical criteria include calibration, predictive parity, and error rate balance. While these criteria reflect our expectations of a fair decision procedure, tensions have emerged between them.

First, it has been observed that an algorithm can satisfy some of these criteria while failing others, leading to potentially conflicting interpretations of whether the algorithm is fair, as illustrated in the COMPAS case, an algorithm used to predict recidivism. Subsequently, it has been proved that not only does an algorithm not need to satisfy the most prominent fairness criteria at the same time but it cannot satisfy them simultaneously except under special circumstances that are unlikely to occur in real-life settings where our algorithms are deployed. These results, known as the \textit{impossibility results}, elicited various reactions. Some scholars interpreted the impossibility results as the inevitability of bias or the impossibility of complete neutrality towards sensitive traits. Others, on the other hand, have challenged the relevance or importance of some fairness criteria. 


\section{The impossibility results and the reactions}
%The impossibility results and the reactions
%The impossibility results and the reactions
%The impossibility results and the reactions
%The impossibility results and the reactions
\subsection{The impossibility results}

I begin by introducing the formal framework in which the fairness criteria and the impossibility results will be articulated. The setting for algorithmic fairness is often defines as follows:

\begin{enumerate}
  \item A profile of sensitive traits $a_i = {a_i^1,..., a_i^n}$. \\
  Sensitive traits may involve gender, race, ethnicity, religion, merital status, age.    
   \item A profile of permissible traits $x_i = {x_i^{n+1},..., x_i^m}$ \\
   Permissible traits may involve educational background, work experience, technical skills, test scores, credit score.
    \item A desired characteristic, $y_i \in \{0, 1\}$  \\
    The desired characteristic can be qualification for a job, aptness for a training program, or a propensity to exhibit some desired behavior (such as repaying credit, avoiding recidivism, or adhering to professional standards). 
    \item A score  $s_i \in [0, 1]$ assessing the likelihood that $i$ has the desired characteristic. 
    \item A decision, $\delta_i \in \{0, 1\}$  \\
    The decision is the outcome of an evaluative process, such as whether an individual is hired for a job, admitted to a program, granted a loan, awarded a scholarship, selected for a leadership position, or approved for housing, or granted parole. 
    \item A threshold score $t \in [0, 1]$ used to determine whether individual $i$ is deemed to possess the desired characteristic.
\end{enumerate}


\begin{definition}
  \textup{\textbf{ (Calibration) } \\
  A score $s$ is well calibrated if it reflects the same likelihood of having the desired characteristic irrespective of group membership. That is, if for all values of $s$ and each pair of groups $a_i, a_j$:  } 
  \[ P(Y=1|S=s, a_i) = P(Y=1|S=s, a_j) \] 
\end{definition}

  \begin{definition}
    \textup{\textbf{ (Predictive Parity) } \\
    A score $s$ satisfies \textit{predictive parity} at a threshold $t$ if the likelihood of having the desired characteristic is the same among individuals whose scores are above the threshold, regardless of group membership. That is, if for each pair of groups $a_i, a_j$:} 
    \[ P(Y=1|S > t, a_i) = P(Y=1|S > t, a_j) \] 
  \end{definition}

  \begin{definition}
      \textup{\textbf{ (Error rate balance) } \\
      A score $s$ satisfies \textit{error rate balance} at a threshold $t$ if the false positive and false negative error rates are equal accross groups. That is, if each pair of groups $a_i, a_j$:}\\
    \begin{align}
      P(S > t|Y=0, a_i) = & P(S > t|Y=0, a_j) \\
      P(S \leq t|Y=1, a_i) = & P(S \leq t|Y=1, a_j)
    \end{align}
 \end{definition}


 \begin{definition}
  \textup{\textbf{ (Statistical Parity) } \\
  A score $s$ satisfies \textit{statistical parity} at a threshold $t$ if the proportion of individuals whose scores are above the threshold are equal accross the groups. That is, if each pair of groups $a_i, a_j$:}\\
\[   P(S > t| a_i) = P(S > t|a_j) \] 
\end{definition}

\begin{definition}
  \textup{\textbf{ (Balance for the negative and positive classes) }} \\
  \textup{A score $s$ satisfies balance for the negative and positive classes if it satisfies \ref{balancenegclass} and \ref{balanceposclass}}


\begin{subdefinition} \label{balancenegclass} \textup{A score $s$ satisfies 
   \textit{balance for the negative class} the average score assigned to individuals in one group who do not have the desired characteristic must be the same as the average score assigned to individuals in the other group who do not have the desired characteristic. That is, for each pair of groups $a_i, a_j$:}
   \[
\mathbb{E}[s_i \mid Y = 0, a_i] = \mathbb{E}[s_j \mid Y = 0, a_j]
    \]
\end{subdefinition}


  \begin{subdefinition}  \label{balanceposclass} 
 \textup{A score $s$ satisfies 
    \textit{balance for the positive class} the average score assigned to individuals in one group who have the desired characteristic must be the same as the average score assigned to individuals in the other group who have the desired characteristic. That is, for each pair of groups $a_i, a_j$:}
    \[
 \mathbb{E}[s_i \mid Y = 1, a_i] = \mathbb{E}[s_j \mid Y = 1, a_j]
     \]
\end{subdefinition}

\end{definition}

Kleinberg et al. \citeyear{Kleinberg_2016} prove that no algorithm can satisfy balance for the negative and positive classes and error rate balance unless either (i) base rates of the desired characteristics are equal across the relevant groups, or (ii) the algorithm is a perfect predictor---i.e. assigns a score of 1 to all those who have the desired characteristic and a score 0 to all those who do not have the desired characteristic. Chouldechova \citeyear{Chouldechova_2017} a related result that algorithm cannot satisfy Predictive Parity and Error rate balance at the same time unless the unless base rates are equal or the algorithm is a perfect predictor. 

Predictive parity is important because: .... \\
Error rate balance is important because .... \\
Calibration is important because ....  \\
Balance in the positive and negative classes is important because \\

\subsection{Responses to the impossibility results}

A common reaction to impossibility results is to evaluate which fairness criteria are most important and indispensable. Such efforts can include: 
\begin{enumerate}
    \item Trying to decide between PPV and error rate balance. \label{PPVerrorratetradeoff}
     \item Trying to weaken PPV and/or error rate balance. 
    \item Turning to individual metrics-- This is also not satisfting. My model will show why individual metrics alone cannot guarentee fairness. 
    \item Challenging what is considered a necessary condition for fairness by examining algorithms purported to be fair. \label{reject}
\end{enumerate}

Before discussing these suggestions, it's important to highlight that the impossibility results in algorithmic fairness literature are challenging because we are under non-ideal conditions. In an ideal scenario, if we had perfect algorithms assigning risk scores of 0 to individuals without the desired characteristic and 1 to those with it, achieving fairness would be straightforward and would not require any trade-offs. However, in practice, our algorithms are far from perfect, leading to trade-offs and difficulties in satisfying multiple fairness criteria simultaneously. Therefore, although the tensions between fairness criteria are indeed seems to be inevitable in real life settings, calling these trade-offs inherent risks obscuring the role that practical limitations play in creating them. 

\textit{1. Trade-off}

One response to the impossibility results is relaxing PPV, positive error rate balance, or negative error rate balance, as suggested in \ref{PPVerrorratetradeoff}. Chouldechova \citeyear[p. 161]{Chouldechova_2017} points out that we can tune these signifiers in any of the following ways:

(i) Allow unequal False Negative Rates while maintaining equal Positive Predictive Values and achieving equal False Positive Rates.
(ii) Allow unequal False Positive Rates while maintaining equal Positive Predictive Values and achieving equal False Negative Rates.
(iii) Allow unequal Positive Predictive Values while maintaining equal False Positive Rates and False Negative Rates.


Chouldechova \citeyear[p. 161]{Chouldechova_2017} argues that (iii) can be a preferred approach in some cases. This response does not satisfy many people because PPV is still seen as a fairness criteria. SOME REASONS: ... 

At any rate, trying to decide between PPV and error rate balance is not a satisfactory way. 


\textit{2. Weaken}

Another approach has been instead of giving up on PPV, weakening it. Some people have suggested base-rate tracking as a weakening of calibration. This is a promisin aproach: however, weakenings of PPV are also subject to impossibility results. (Steward)

\textit{3. Turn to individual metrics}

Instead, some has turned to individual metrics of fairness giving up on group fairness. This is also not satisfying. I will show that individual metrics are not sufficient for fairness. I will explain this later. 

\textit{4. Reject the criteria}

An example of the efforts characterized by \ref{reject} is performed by Brian Hedden. He considers an algorithm which he argues is fair and rejects all the metrics altogether except calibration: arguing that none of them are necessary for a fair algorithm. 

The setting Brian Hedden considers is the following: Each individual in a population is randomly assigned a coin with  varying biases. Then the individuals are randomly assigned to two rooms, A and B. The goal is to predict whether each personâ€™s coin lands heads or tails. Each coin comes labeled with its bias---i.e. its objective chance of landing heads which is a real number in the interval [0,1]. 

Hedden suggests that the following is a perfectly fair and unbiased predictive algorithm in this setting: For each person, read their coinâ€™s bias label and assign them a score equal to the label value. That is, if the bias label says 'ð‘¥', assign that person a score of ð‘¥. Set a threshold at 0.5: if $ð‘¥>0.5$, predict they are a heads person (positive), and if $x<0.5$, predict they are a tails person (negative). Assume no coin has a bias of exactly $x=0.5$, though this assumption can be avoided by making an arbitrary or randomized prediction in such cases.

Hedden stresses that this is algorithm "is not unfair to any people in virtue of their room membership" because its predictions are not sensitive to individualsâ€™ room membership. "And the sole feature on which its predictions are based (the labeled bias of the coin) is clearly the relevant one to focus on and is neither a proxy for, nor caused or explained by, room membership." "Indeed, it is not just that the algorithm is in no way unfair to individuals in virtue of their membership in a certain room; there is seemingly no unfairness of any kind anywhere in this situation. Moreover, this algorithm is uniquely optimal; no alternative can be expected to do as well or better at predicting whether individuals are heads people or tails people."


--- Hedden argues that calibration cannot be violated. 

--will calibration also be satisfied:::: there are many problems with Hedden's setting.  

Hedden makes the following observation: all fairness criteria except calibration can be violated simultaneously and even when base rates are equal between the two rooms. (p. 221). Really the striking aspect of Hedden's observation that this fair looking algorithm can violate fairness criteria even when base rates are equal which is seen as the root of the fairness problems (at least the difficulty in satisfying statistical criteria).


See footnote 15 and 35-- calibration need to be satisfied purely because he defines it in terms of expectation. but if we define it as .. He assumes that actual frequencies match the expectation. 


He considers the following situation (p. 221):  

Suppose that the actual relative frequencies match coin biases. Room A contains twelve people coins labeled   `0.75' and eight people with coins labeled `0.125'. The former are assigned the score 0.75, therefore, predicted to be heads people; and nine of them turn out to be really heads people. The latter are all assgined a score 0.125, therefore, predicted to be tails people, and but one of them turns out to be a heads person. In room B, there are ten people with coins labeled `0.6' and then with people with coins labeled `0.4'. The former are assigned a score of 0.6, therefore, predicted to be heads people, and six of them turn out to be really heads people. The latter are all assigned risk score 0.4, therefore predicted to be tails people, yet four of them turn out to be heads people. 

Hedden takes this to be a counterexample to the necessity of the statistical criteria except calibration. He points out that calibration cannot be violated in this set-up. This is because he defines probabilities as "probabilistic expectations" as opposed to actual relative frequencies (see footnote 15, p. 214). However, he doesn't mean epistemic probability or credence by this at least, in the coin flip case, he suggests that probabilistic expectation can be understood as objective chance. "In others, where no objective chanciness is involved, I suggest that they are best understood as epistemic probabilities, and more specifically as the subjective probabilities that would be assigned by a reasonable individual who is familiar with the workings of the algorithm in question"(see footnote 15, p. 214). 

I believe that the main reason we perceive Hedden's algorithm to be fair is the interpretation of probability as objective chances. If the scores assigned by the algorithm did not track objective chances but only represented epistemic probabilities, we would not deem this fair: it would be simply be more informative about one group and less for another. It would be using an estimator that is noisier for the group---without any excuse. 

Even in the objective chance reading, the algorithm uses an algorithm that is noisier for a group. Nevertheless, one may argue that this is not the fault of the algorithm but the world is kinder to one group and not to the other. One can still force the noise interpretation: if the bias of the coin is not the only determinant of whether the coin lands heads or tails, by not considering that the algorithm misses more information on one group than on another. But if we interpret the objective chances strictly, there is really nothing the algorithm can do than looking at the coin bias. Brian Hedden's claim holds: there is nothing we can do to improve the algorithm. (notice that if there are other factors, there is a way to improve the algorithm). I will wrap this and assume that Hedden's algorithm is indeed fair in its setting. 

Again, what drives the result is inframarginality, as Hedden also points out. Inframarginality is pointed out to undermine the fairness criteria. However, it has not been seen as a source of or a result of unfairness. Here I will argue, unless the inframarginality in the scores tracks a the inframarginality in the objective chances, it is a result of unfairness in the procedure that perpetuates the bias. Now, the question is whether in the real-life settings that the algorithms are deployed there is a difference in the objective chance distribution. This requires answering two questions affirmatively: 1) Are there objective chances in the relevant setting? 2) If yes, are there any differences in the objective chance distributions so that objective probability is a noisier estimator for one group? 

Many traits the algorithms are looking for are not chancy, and it is questinable whether any is a chancy event. Even if some of the events are in the future, this doesn't make them chancy. However, I will grant this. Is there any reason to suspect that the chance distribution will be more marginal in one group? If we can rule this out, then we can say... 

Noticing the importance of the objective chance distribution also diminishes the strikingness of Hedden's result: he shows that error rate balance and predictive parity are not satisfied even if the base rate is satisfied. However, the chance distribution is significantly different. In this setting objective chances are more important than the ... Thus, we should not read Hedden's result as even if the groups are significantly similar the fairness criteria can be violated. 

To take stock: Inframarginality is not an excuse unless objective chances are involved in the relevant setting and the objective chance distibution of groups have inframarginality differences. However, we have no good reasons to think that is the case. 

ViganÃ² et al. \citeyear{Vig2022} provide another argument based on the kind of probabilities and the relevancy of Hedden's setup to most of real-life algorithmic settings. They try to avoid any commitment about (ontological) interpretation of probabilities and instead talk about ways of determining probabilities. They distinguish between two ways of computing probabilities: individual-based and group-based. "An h-individual probability is determined with data of the person whom the prediction is about; an h-group probability is determined with data of other people beyond the person whom the prediction is about. These two types of h-probabilities correspond to the two types of probability-based decision-making practices: an h-individual practice is one in which we make a decision about a person by employing the h-individual probabilities attributed to that person; an h-group practice is one in which we take a decision about a person by employing the h-group probabilities attributed to that person"(ViganÃ² et al., 2022, p. 2). They argue that Hedden's argument is valid for individual-based probabilities but does not apply to group-based probabilities about humans. Crucially, "Many practices in data science involve h-group probabilities. This includes the COMPAS example that Hedden treats as paradigmatic."

Their main point is that there is a moral difference to individual- and group-based probabilities. They argue that making decisions on group-based probabilities pro tanto wrongs people because this doesn't treat people as individuals. 

â€œH-individual practices are based on h-individual probabilities, which are estimated only on the basis of what the person did (e.g., being late or punctual, thrifty or spendthrift) or other characteristics (e.g., financial independence, stability of employment, having dependent children) that do not require comparisons with the outcomes of other people to reasonably inform a guess about the future outcomes of that person. Thus, h-individual practices treat a person as an individual. H-group practices are based on h-group probabilities, which are founded on the attribution of the person to a specific group sharing the same or similar aspects with that person (e.g., being a woman, a bank clerk, a member of the same age group, punctual). Thus, h-group practices treat the person as an instance of a more general type, not as an individual.â€ ([ViganÃ² et al., 2022, p. 7](zotero://select/library/items/TPA9HN24))

Thus, Hedden's model is not relevant to real machine learning problems where fairness is a question. 

%%%%%
%%%%%
SÃ¸gaard et al. \citeyear{Sogaard_2024} contend that Hedden's thought experiment is irrelevant to real machine-learning problems on different grounds. They argue as follows: The performance of classification models is evaluated using hold-out samplesâ€”-randomly selected subsets of the dataset excluded from the estimation process. In that, the error of the model is defined as the probability of failing to correctly predict the label for a random data point drawn from the underlying distribution. This principle extends to fairness evaluations: Fairness metrics quantify the expected properties of the underlying distribution, particularly whether the distributions of predictions are consistent across subgroups: ``Models are fair if subgroups incur the same loss in the limit" \cite[p. 9]{Sogaard_2024}. In short, fairness in ML is evaluated over distributions, not finite samples, whereas Hedden measures fairness on a specific, small sample of data. 

Since in ML held-out samples approximate performance on data distributions, and fairness metrics assess whether subgroup performance converges as more data is sampled independently and at random. To demonstrate flaws in fairness metrics, Hedden would need to show that these metrics consistently score subgroups differently as new data is sampled from the underlying distribution. 

SÃ¸gaard et al. \citeyear{Sogaard_2024} note that if the classification problem Hedden considers were a genuine ML problem, his algorithm would be evaluated on additional data sampled independently from the underlying distribution.

The fairness of the algorithm would then be assessed based on its loss in the limit, as the sample size approaches infinity. In this scenario, biases in the subgroups would converge to \(0.5\) as he assumes that base rates are equal across groups and and all fairness metrics would classify the optimal model as fair. This result follows from the Central Limit Theorem, which states that for a population with mean \(\mu\) and standard deviation \(\sigma\), the distribution of sample means from sufficiently large random samples (with replacement) will approximate a normal distribution with mean \(\mu\) and standard deviation \(\sigma\). 


Any response trying to argue that the future samples from the underlying distribution will be similar to the sample Hedden considers would need to admit that coin distribution or the distribution of people to rooms is non-random. However, as SÃ¸gaard et al. \citeyear{Sogaard_2024} write "when coin assignment is non-random, our intuition flips: People in Room 1 are assigned coins from a different probability distribution, which means that the overall distribution of coins is systematically biased â€“ and then Heddenâ€™s algorithm becomes unfair for ignoring this systematic bias." 

One may still try to defend that Hedden's algorithm is fair: surely, it reproduces the underlyting systematic bias, but it does not have its own bias, so to speak. However, the algorithm uses a measure which is systematically noisier and less informative of one of the groups. 

SÃ¸gaard et al. conclude that Hedden's algorithm is not relevant to real ML problems. However, an interesting lesson of their observation is that any modification to Hedden's thought experiment that would render it relevant to ML problems and retain our intiuition that the algorithm is fair would satisfy the fairness criteria provided that he assumes the base rates are equal. (We don't have to assume this even, if the distibutions are random, it will be equal.)
%%%%

1) What this shows is limited because still in the long run, if the assignments are really random, other criteria must be satisfied. 
2) Objective chances and coins: the probability distributions are different in different groups. However, this time, this difference of probability distributions drive the result and not the base rate. If we are committed to objective chance distributions, equality of these distributions is more important. 
3) Looking at probabilities is a more noisy estimator for room B. 

Do we have a good reason to assume that the probability distributions would be different if the base rates were equal? I don't think so. 


Although I believe Hedden's model is not relevant to the context of algorithmic decision-making, his approach is a good starting point. He writes: â€œOne way to determine whether some criterion is a genuine necessary condition on algorithmic fairness is to find a perfectly fair algorithm and see whether it is possible for it to violate that criterion. If so, then the criterion is not in fact a necessary condition on algorithmic fairness.â€ I believe Heddenâ€™s proposed fair algorithm is not fair in the context of individuals or at least, it is not relevant. (Importantly, Heddenâ€™s algorithm assumes equal base rates) However, there is some merit to his suggestion to start from a perfectly fair algorithm. This is what I will do in this paper. I will consider a fair algorithm and look at which conditions it satisfies. But how a fair algorithm must look requires some theoretical speculation.


\section{A fair model}

It is common to define fairness which traits are allowed in the decision procedures and which are not. A common idea about fairness is the following: 

 \textbf{Anti-classification:} The decision or evaluation of an algorithm should not depend on individuals's sensitive traits. 

This principle is often made more precise as folows: if two individuals with the same permissible traits receive the same decision, $x_i = x_j \Rightarrow \delta_i = \delta_j $ \cite[p. 8]{Patty_Penn_2023}. I will not use this formulation because this does not guarantee fairness unless other conditions about the permissible traits are satisfied. I will stick to the negative and, temporarily vague, formulation. 

Anti-classification is often taken as given; however, I believe it is fruitful to ask why it is taken as a pillar of a fair algorithm. Why do we consider involvement of the sensitive traits to be unfair? I believe, at least one underlying reason is another intuition about fairness: similarly qualified individuals must be treated similarly. The problem is of course, sometimes the decision-makers don't know whether two individuals are similarly qualified because qualification (the desired characteristic)is unobservable. We can then hedge this principle as follows: treat individuals who are known to be similarly qualified similarly.

\begin{definition} \\
  \textup{ \textbf{(Equal Treatment)}}  \\
$ P(D|Y, G) =  P(D|Y, \neg G)$ \textup{when the value of Y is known at the time of decision}
\end{definition}

The problem is often we don't know whether a person is qualified or not. Therefore, the same equation is often interpreted as equal error rates. This interpretation assumes that treating equally qualified individuals equally as a guiding ideal. 

Notice that if an algorithm is perfect predictor, it will be subject to this requirement.  

Perfect predictor is required for this but it is not sufficient. We may know that someone is qualified but not hire them on taste-based discrimination. 

However, when Y is not known, holding on to the same equation might be too strong. Instead, we must hold on to a principle as follows: 

\begin{definition} \\
  \textup{ \textbf{(Equal Treatment, probabilistic)}}  \\
$ P(D|S=s, G) =  P(D|S=s, \neg G)$ \textup{when the value of Y is not known at the time of decision}
\end{definition}

Now, this probabilistic equal treatment condition underlies only one part of anti-classification. Namely, anti-classification about decision: \\


\textbf{Anti-classification about decision:} Sensitive traits should not be involved in decision making only the probability estimation must be involved. \\

Anti-classification has another part, which can be called anti-classifcation about estimation: \\

\textbf{Anti-classification about estimation:} Sensitive traits should not be involved in estimating the risk scores. \\

Before discussing anti-classification about estimation, it should be noted that equal treatment both in perfect and probabilistic versions, ground more than anti-classification based on sensitive traits: if it is knownn whether an individual is qualified or now, the decision must be based only on this. Nothing else, not only sensitive traits but any other trait the individual may have is irrelevant-- whether that trait be one's past training or past experience or horoscope sign. If the qualification is known, they are irrelevant (if they are not, then they must be considred to be a part of the desired characteristic the algorithm is looking for) A similar point can be made about the probabilistic version of this principle.

However, this doesn't explain the contrast between the sensitive traits and the permissible traits. If two people to be equally qualified, then we must treat them in the same way, not only regardless of their sensitive traits but permissible traits as well. In other words, once we have the information about whether an individual has the desired characteristic, anything else is irrelevant. Therefore, if we know the value of

When we move to anti-classification about estimation. This is also considered to be given; but it is not. Why is it unfair to base the estimation about whether one is qualified or not on the sensitive traits? It doesn't follow from anti-classification about decision or the principle that grounds it, the equal treatment. I think it is because we assume that those traits are irrelevant to whether one is qualified or not. When we are trying to estimate whether one is qualified or not, we look at other characteristics---such as education, credit score, etc. Here, too, we think that it is inappropriate to base the estimations on gender--while it is okay to base estimations on some other characteristics. But why? Is it because these are sensitive traits? What makes them sensitive? History of discrimination or even history of oppression is not a sufficient answer. This can make these characteristic particularly salient, but I think that is not that big a problem. These are problematic because the discrimanation was unfair even back then. What makes discrimination on the basis of education etc acceptable even desired while discrimination on the basis sensitive traits is a principle like the following:



\begin{definition}
\textup{\textbf{ (Anti-essentialism) } \\
There is nothing \textit{inherent} about oneâ€™s sensitive traits that makes one better or worse qualified. Therefore, any correlation between possessing these sensitive traits and qualification must be driven either by confounding factors or by mediators.}
\end{definition}

This doesn't explain why these are the sensitive traits. There are many traits that are not relevant to one's qualifications: the favourite ice-cream flavour, zodiac sign, etc.. If it turns out that there is a correlation between these characteristics and qualification, we would look for confounding factors or mediators. 

I think this anti-essentialism partly underlies the anti-classification about estimation. Sometimes this intuition made precise as conditional demogratic parity. 

\begin{definition}
  \textup{\textbf{ (Conditional demographic parity) } \\
  \indent
  An algorithm satisfies \textbf{conditional demographic parity} if, for any pair of profiles of sensitive traits}, $a_i$,  $a_j$, \\
  
  \textup{If } $x_i = x_2$\textup{, then } $P(\delta_i|a_i, x_i) = P(\delta_j|a_j, x_j)$
  \end{definition}


However, just saying that the sensitive traits should not be involved is not enough to guarantee even minimal fairness. The characteristics that are involved must be relevant to the qualification. 

In endorsing conditional demographic parity as a fairness criteria an implicit assumption must be that these are equally good indicators of the deserved quality for both groups. If x is a good indicator for one group but bad for the other, then conditional demographic parity does not reflect fairness. Thus, conditional demographic parity is an indicator of fairness only if another condition is satisfied:

\begin{definition}
  \textup{\textbf{ (Equally good indicator) } \\
  \indent
  An algorithm satisfies \textbf{Equally good indicator} if, for any pair of profiles of sensitive traits}, $a_i$,  $a_j$, \\
  
  \textup{If } $x_i = x_2$\textup{, then } $P(Y_i|a_i, x_i) = P(Y_j|a_j, x_j)$
  \end{definition}

However, an implication of the impossibility results is that equally good indicator and conditional demographic parity cannot be satisfied at the same time unless the base rate is equal across groups or the algorithm is a perfect predictor. In the next section, I will show more about the satisfaction conditions of these principles which I will believe instructive. Namely, it is not only that these conditions are simultenously satisfied only under special conditions, but even when they are considered separately they are satisfied under rare conditions. %%% This should tell us more about how to interpet these results. 

I argue for a weaker fairness condition: If there is a test, it should not be depend anything else than whether one is qualified or not. The test should treat equally qualified individuals equally. One implication of this condition is the following:

\begin{definition}
  \textup{\textbf{ (Fair Test) } \\
  \indent
  An algorithm satisfies \textbf{fair test} if, for any pair of profiles of sensitive traits}, $a_i$,  $a_j$, \\
  (If there is any test involved... )
  \\
  
  \textup{If } $y_i = y_2$\textup{, then } $P(t_i|a_i, y_i) = P(t_j|a_j, y_j)$
  \end{definition}

(Fair test is sometimes understood as an equally good indication of success. this is exactly what we don't have due to the impossibility results.)

Anti-essentialism, fair test, anti-classification gives us the following Bayesian networks modelled in \ref{fig:fair}. A Bayesian Network represents a set of variables and their conditional independences by organizing them into a \textit{Directed Acyclical Graph} (DAG).  A DAG is comprised of a set of nodes and a set of arrows between (some of) the nodes and is constrained by the condition that one cannot form a cycle by following the arrows. The nodes represent the variables and the arrows represent the probabilistic dependence relations between the variables. If two nodes are linked by an arrow, the node at the tail is called the parent node (of the node at the head) and the node at the head is called the child node (of the node at the tail).  A node at a tail is called a descendant node of another node if the former can be reached by following arrows starting from the latter. That is, if the the former is a child node of the latter, or a child node of a child node of the latter, and so on. 

The arrows in a Bayesian Network carry information about the independence relations between the variables in the network. Parental Markov Condition (PMC) expresses this information:

\begin{definition}{Parental Markov Condition (PMC):}
\textup{For each variable represented by a node on a Bayesian Network, the variable is probabilistically independent of all variables represented by its non-descendent nodes, conditional on all variables represented by its parent nodes.}
\end{definition}


Given PMC, we can read off the following information from the Bayesian networks in figure \ref{fig:fair}:

\setcounter{equation}{0}
\ref{fig:fair}a: \\

\begin{align}
  D \perp & Y, E, G | T \\
  T \perp & E, G | Y \\
  Y \perp & G | E
\end{align}


\ref{fig:fair}b: \\

\begin{align}
  D \perp & Y, G | T, E \\
  T \perp & E, G | Y \\
  Y \perp & G | E
\end{align}

1 and 4 ensure that anti-classification about decision is satisfied. They imply that given the values of T and E, the decision is independent of the group membership. 2 and 5 ensure that fair test condition is satisfied, given the value of Y, the test result doesn't depend on anything else, including group membership. 3 and 6 tells us whatever is included in E exhausts the mediators that mediate the relation between G and Y. That there exists a set of traits which fully mediate this relation is an implication of anti-essentialism. This condition is especially important for the second model (\ref{fig:fair}b:) because ???? Anti-essentialism is not a fairness criteria but it is an assumption we implicitly make in designating certain characteristics as sensitive. Therefore, they might be context dependent. For example, age or pregrancy status can be a sensitive characteristic in job applications or credit decisions, but they may not be a sensitive trait in medical contexts. I think this intuition partly explain the role of anti-essentialism. Anti-essentialism tells us we can explain the correlation between Y and G completely through mediating factors. Fair test, if one is --- one's test result should not depend anything else other than one is qualified or not. Not every algorithm relies on a test. 


    
  \begin{figure}[h]
\begin{tikzpicture}[
roundnode/.style={circle, draw=black, minimum size=10mm},
]
%FIT 
%Nodes
\node[roundnode]    (G)     {G};
\node[roundnode]    (E)    [below=of G] {E};
\node[roundnode]    (Y)     [below=of E] {Y};
\node[roundnode]    (T)    [below=of Y] {T};
\node[roundnode]    (D)     [below=of T] {D};
\node[node distance=2mm]    (a)     [below=of D] {(a)};
%Lines
\draw[->] (G.south) -- (E.north);
\draw[->] (E.south) -- (Y.north);
\draw[->] (Y.south) -- (T.north);
\draw[->] (T.south)-- (D.north);
%\draw[->] (E.east) to[bend left] (D.east);


%Nodes
\node[roundnode]    (G')     [right=5cm of G]{G};
\node[roundnode]    (E')    [below=of G'] {E};
\node[roundnode]    (Y')     [below=of E'] {Y};
\node[roundnode]    (T')    [below=of Y'] {T};
\node[roundnode]    (D')     [below=of T'] {D};
\node[node distance=2mm]    (b)     [below=of D'] {(b)};
%Lines
\draw[->] (G'.south) -- (E'.north);
\draw[->] (E'.south) -- (Y'.north);
\draw[->] (Y'.south) -- (T'.north);
\draw[->] (T'.south)-- (D'.north);
\draw[->] (E'.east) to[bend left] (D'.east);

%Nodes
\node[roundnode]    (G'')     [right=5cm of G']{G};
\node[roundnode]    (E'')    [below=of G''] {E};
\node[roundnode]    (Y'')     [below left=of E''] {Y};
\node[roundnode]    (D'')     [below right=of E''] {D};
\node[node distance=2mm]    (c)     [below=2cm of E''] {(c)};
%Lines
\draw[->] (G''.south) -- (E''.north);
\draw[->] (E''.south west) -- (Y''.north east);
\draw[->] (E''.south east)-- (D''.north west);

\end{tikzpicture}
 \caption{Bayesian Networks. Prima Facie Fair Decision Procedures}
  \label{fig:fair}
\end{figure}


This model is at least prima facie fair. There is another alternative. Where decision takes E into consideration. 


Consider \ref{fa,fig:fair}a: 

Since this is a Bayes net 







Anti-classification about decision: 
First, group membership does not influence the decision, ensuring that the procedural fairness condition is satisfied.
Anti-classification about estimation: 
Second, the group membership is not involved in estimating the probability of Y: if E and T. We know this is possible beacause of anti-classification. 
Third, the test is fair; it is not influenced by anything other than whether the individual is qualified or not. 

In this model the relationship between G, E, and Y are external to the algorithm. However, we can assume that there is a mediator between G and Y given anti-essentialism. T can be any test the algorithm is using, such as SAT score. The decision must be either based on \dots

What if there is no test: it is also fine. 

Compare these models with unfair procedures. 




\begin{figure}[h]
  \begin{tikzpicture}[
  roundnode/.style={circle, draw=black, minimum size=10mm},
  ]
  %FIT 
  %Nodes
  \node[roundnode]    (G)     {G};
  \node[roundnode]    (E)    [below=of G] {E};
  \node[roundnode]    (Y)     [below=of E] {Y};
  \node[roundnode]    (T)    [below=of Y] {T};
  \node[roundnode]    (D)     [below=of T] {D};
  \node[node distance=2mm]    (a)     [below=of D] {(a)};
  %Lines
  \draw[->] (G.south) -- (E.north);
  \draw[->] (E.south) -- (Y.north);
  \draw[->] (Y.south) -- (T.north);
  \draw[->] (T.south)-- (D.north);
  \draw[dashed] (E.east) to[bend left] (D.east);
  \draw[->] (G.west) to[bend right] (D.west);
  
  
  %Nodes
  \node[roundnode]    (G')     [right=5cm of G]{G};
  \node[roundnode]    (E')    [below=of G'] {E};
  \node[roundnode]    (Y')     [below=of E'] {Y};
  \node[roundnode]    (T')    [below=of Y'] {T};
  \node[roundnode]    (D')     [below=of T'] {D};
  \node[node distance=2mm]    (b)     [below=of D'] {(b)};
  %Lines
  \draw[->] (G'.south) -- (E'.north);
  \draw[->] (E'.south) -- (Y'.north);
  \draw[->] (Y'.south) -- (T'.north);
  \draw[->] (T'.south)-- (D'.north);
  \draw[dashed] (E'.east) to[bend left] (D'.east);
  \draw[->] (G'.west) to[bend right] (T'.west);


  
  \end{tikzpicture}
   \caption{Bayesian Networks. Prima Facie Unfair Decision Procedures}
    \label{fig:unfair}
  \end{figure}


  Results:

\begin{enumerate}
  \item Error rate balance and PPV cannot be satisfied at the same time unless equal base rate or perfect predictor
  \item In a fair algorithm, they are necessarily satisfied if equal base rate or perfect predictor
  \item In an unfair algorithm they need not be satisfied even when equal base rate or perfect predictor, even when both are true (You haven't proved this, just a hunch)
  \item (Though some unfair algorithm can satisfy them)
  \item Conditional on mediators, ... 
  \item Some unfair algorithms fail to satisfy.
  \item However, some unfair algorithms can satisfy them.
\end{enumerate}





This model is silent about the score. It can be interpreted as P(Y|T) or P(Y|E,T). This model satisfies it. 


Full specification of the fair algorithms:

(a)

\begin{align*}
  P(G) = g & \quad \\
  P(E|G) = e & \quad P(E|\neg G) = f \\
  P(Y|E) = b & \quad P(Y|\neg E) = c \\
  P(T|Y) = x & \quad P(T|\neg Y) = y \\
  P(D|T) = \delta & \quad P(D|\neg T) = \gamma
\end{align*}

(b)

\begin{align*}
  P(G) = g & \quad \\
  P(E|G) = e & \quad P(E|\neg G) = f \\
  P(Y|E) = b & \quad P(Y|\neg E) = c \\
  P(T|Y) = x & \quad P(T|\neg Y) = y \\
  P(D|T, E) = \delta \quad P(D|\neg T, E) = \gamma & \quad P(D|T, \neg E) = \zeta \quad P(D|\neg T, \neg E) = \epsilon \\
\end{align*}

(c)

\begin{align*}
  P(G) = g & \quad \\
  P(E|G) = e & \quad P(E|\neg G) = f \\
  P(Y|E) = b &  \quad P(Y|\neg E) = c \\
  P(D|E) = \delta &  \quad P(D|\neg  E) = \epsilon
\end{align*}






Only E is involved (model c):

Predictive parity:

\begin{align*}
  & P(Y|D,G) = \frac{P(Y, D, G)}{P(D, G)} \\
  & = \frac{\sum_{E} P(Y, D, G, T, E) }{\sum_{E, Y} P(Y, D, G, E)} \\
  \scriptscriptstyle{
  & = \frac{
    \begin{aligned}
      & P(D|E)P(Y|E)P(E|G)P(G) 
  + P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
\end{aligned}
 }{ 
  \begin{aligned} 
& P(D|E)P(Y|E)P(E|G)P(G) 
+ P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
  \\ &
   +  P(D|E)P(\neg Y|E)P(E|G)P(G) 
 + P(D|\neg E)P(\neg Y|\neg E)P( \neg E|G)P(G)}
\end{aligned}} \\
& = \frac{\delta beg + \epsion c(1-e)g}{\delta beg + \epsion c(1-e)g \\ &
+ \delta (1-b) e g + \epsilon (1-c) (1-e) g
}\\
& = \frac{g(\delta be + \epsion c(1-e))}{g(\delta be + \epsion c(1-e) \\ &
+ \delta (1-b) e  + \epsilon (1-c) (1-e)) 
}\\
& = \frac{e(\delta b - \epsilon c) +  \epsilon c
}{e(\delta b - \epsilon c) +  \epsilon  
}
\end{align*}



Error rate balance, model c

\begin{align*}
  & P(D|Y,G) = \frac{P(Y, D, G)}{P(Y, G)} \\
  & = \frac{\sum_{E} P(Y, D, G, T, E) }{\sum_{E, D} P(Y, D, G, E)} \\
  \scriptscriptstyle{
  & = \frac{
    \begin{aligned}
      & P(D|E)P(Y|E)P(E|G)P(G) 
  + P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
\end{aligned}
 }{ 
  \begin{aligned} 
& P(D|E)P(Y|E)P(E|G)P(G) 
+ P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
  \\ &
   +  P(\neg D|E)P(Y|E)P(E|G)P(G) 
 + P(\neg D|\neg E)P(Y|\neg E)P( \neg E|G)P(G)}
\end{aligned}} \\
& = \frac{\delta beg + \epsion c(1-e)g}{\delta beg + \epsion c(1-e)g \\ &
+ (1 - \delta) b e g + (1- \epsilon) c (1-e) g
}\\
& = \frac{g(\delta be + \epsion c(1-e))}{g(\delta be + \epsion c(1-e) \\ &
+ (1 - \delta) b e + (1- \epsilon) c (1-e))
}\\
& = \frac{e(\delta b - \epsilon c) +  \epsilon c
}{e(b - c) +  c 
}
\end{align*}








Model A (Only T)

Predictive parity: \\

\begin{proof}
  \begin{align*}
    & P(Y|D,G) = \frac{P(Y, D, G)}{P(D, G)} \\
    & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, Y} P(Y, D, G, T, E)} \\
    \scriptscriptstyle{
    & = \frac{
      \begin{aligned}
        & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) 
    \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
  \end{aligned}
   }{ 
    \begin{aligned} 
  & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G)
   \\ &
   +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \\ &
     +  P(D|T)P(T|\neg Y)P(\neg Y|E)P(E|G)P(G) 
   + P(D|T)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G)
   \\ &
    + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|E)P(E|G)P(G) 
   + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G)}
  \end{aligned}} \\
  & = \frac{\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g}
  {\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g + \\ &
  \delta y(1-b)eg + \delta y(1-c)(1-e)g + \gamma (1-y)(1-b)eg + \gamma (1-y)(1-c)(1-e)g} \\
  & = \frac{g(\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e))}
  {g(\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &
  \delta y(1-b)e + \delta y(1-c)(1-e) + \gamma (1-y)(1-b)e + \gamma (1-y)(1-c)(1-e))} \\
  & = \frac{\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e)}
  {\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &
  \delta y(1-b)e + \delta y(1-c)(1-e) + \gamma (1-y)(1-b)e + \gamma (1-y)(1-c)(1-e)} \\
  & = \frac{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce
  }{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce + \\ &
  \delta ye - \delta ybe + \delta y - \delta yc -  \delta ye + \delta yce + \gamma e 
  - \gamma ye - \gamma be + \gamma ybe + \gamma - \gamma y - \gamma c + \gamma yc - \\ &
  \gamma e + \gamma ye + \gamma ce - \gamma yce} \\
  & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbe + \delta xc - \delta xce + \cancel{\gamma be} - \gamma xbe + \cancel{\gamma c}- 
  \gamma xc - \cancel{\gamma ce} + \gamma xce +
  \cancel{\delta ye} - \delta ybe + \\ &
   \delta y - \delta yc -  \cancel{\delta ye} + \delta yce + \cancel{\gamma e}
  - \cancel{\gamma ye} - \cancel{\gamma be} + \gamma ybe + \gamma - \gamma y - \cancel{\gamma c}- + \gamma yc  \\ &
  - \cancel{\gamma e} + \cancel{\gamma ye} + \cancel{\gamma ce} - \gamma yce} \\
  & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbe + \delta xc - \delta xce - \gamma xbe - 
  \gamma xc  + \gamma xce - \delta ybe + \\ &
   \delta y - \delta yc  + \delta yce +  + \gamma ybe + \gamma - \gamma y + \gamma yc  - \gamma yce} \\
   & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
   {e(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
     + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma } \\
     & = \frac{e(xb(\delta - \gamma) - xc(\delta - \gamma)+ \gamma (b-c) ) + c(x(\delta - \gamma) + \gamma)}
     {e(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
       + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma }
\end{align*}

 \begin{align*}
 & P(Y|D,\neg G) = \frac{P(Y, D, \neg G)}{P(D, \neg G)} \\
    & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, Y} P(Y, D, \neg G, T, E)} \\
    \scriptscriptstylf{
    & = \frac{
      \begin{aligned}
        & P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
    + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) 
    \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
   + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
  \end{aligned}
   }{ 
    \begin{aligned} 
  & P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
   + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
   \\ &
   +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
    + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \\ &
     +  P(D|T)P(T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) 
   + P(D|T)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G)
   \\ &
    + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) 
   + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G)}
  \end{aligned}} \\
  & = \frac{\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g)}
  {\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g) + \\ &
  \delta y(1-b)f(1-g) + \delta y(1-c)(1-f)(1-g) + \gamma (1-y)(1-b)f(1-g) + \gamma (1-y)(1-c)(1-f)(1-g)} \\
  & = \frac{(1-g)(\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f))}
  {(1-g)(\delta xbf+ \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &
  \delta y(1-b)f + \delta y(1-c)(1-f) + \gamma (1-y)(1-b)f+ \gamma (1-y)(1-c)(1-f))} \\
  & = \frac{\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f)}
  {\delta xbf+ \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &
  \delta y(1-b)f + \delta y(1-c)(1-f) + \gamma (1-y)(1-b)f+ \gamma (1-y)(1-c)(1-f)} \\
  & = \frac{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
  \gamma xc - \gamma cf + \gamma xcf
  }{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
  \gamma xc - \gamma cf + \gamma xcf + \\ &
  \delta yf - \delta ybf + \delta y - \delta yc -  \delta yf + \delta ycf + \gamma f 
  - \gamma yf - \gamma bf + \gamma ybf + \gamma - \gamma y - \gamma c + \gamma yc - \\ &
  \gamma f + \gamma yf + \gamma cf - \gamma ycf} \\
  & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbf + \delta xc - \delta xcf + \cancel{\gamma bf} - \gamma xbf + \cancel{\gamma c}- 
  \gamma xc - \cancel{\gamma cf} + \gamma xcf +
  \cancel{\delta yf} - \delta ybf + \\ &
   \delta y - \delta yc -  \cancel{\delta yf} + \delta ycf + \cancel{\gamma f}
  - \cancel{\gamma yf} - \cancel{\gamma bf} + \gamma ybf + \gamma - \gamma y - \cancel{\gamma c}- + \gamma yc  \\ &
  - \cancel{\gamma f} + \cancel{\gamma yf} + \cancel{\gamma cf} - \gamma ycf} \\
  & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbf + \delta xc - \delta xcf - \gamma xbf - 
  \gamma xc  + \gamma xcf - \delta ybf + \\ &
   \delta y - \delta yc  + \delta ycf +  + \gamma ybf + \gamma - \gamma y + \gamma yc  - \gamma ycf} \\
   & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
   {f(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
     + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma } \\
     & = \frac{f(xb(\delta - \gamma) - xc(\delta - \gamma)+ \gamma (b-c) ) + c(x(\delta - \gamma) + \gamma)}
   {f(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
     + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma }
 \end{align*}

Predictive parity: 
 \begin{align*}
  & P(Y|D,G) =  \frac{e(xb(\delta - \gamma) - xc(\delta - \gamma)+ \gamma (b-c) ) + c(x(\delta - \gamma) + \gamma)}
  {e(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
    + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma } \\
    & P(Y|D,\neg G) = \frac{f(xb(\delta - \gamma) - xc(\delta - \gamma)+ \gamma (b-c) ) + c(x(\delta - \gamma) + \gamma)}
   {f(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
     + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma }
 \end{align*}
\end{proof}


Error rate balance: 

\begin{proof}
  
  \begin{align*}
    & P(D|Y,G) = \frac{P(D, Y, G)}{P(Y, G)} \\
    & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, D} P(Y, D, G, T, E)} \\
    \scriptscriptstyle{
    & = \frac{
      \begin{aligned}
        & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) 
    \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
  \end{aligned}
   }{ 
    \begin{aligned} 
  & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G)
   \\ &
   +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \\ &
     +  P(\neg D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
   + P(\neg D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G)
   \\ &
    + P(\neg D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
   + P(\neg D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)}
  \end{aligned}} \\
  & = \frac{\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g}
  {\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g + \\ &
(1-	\delta ) xbeg + (1-\delta ) c(1-e)g + (1-\gamma ) (1-x)beg + (1- \gamma) (1-x)c(1-e)g} \\
  & = \frac{g(\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e))}
  {g(\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &(
1-	\delta) xbe + (1-\delta ) xc(1-e) + (1-\gamma ) (1-x)be + (1-\gamma ) (1-x)c(1-e))} \\
  & = \frac{\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e)}
  {\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &
  (1-\delta) xbe + (1-\delta )xc(1-e) + (1-\gamma )(1-x)be + (1-\gamma ) (1-x)c(1-e)} \\
  %%%%%%%%
  & = \frac{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce
  }{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce + \\ &
  %%%%second half
  xbe - \delta xbe + xc - \delta xc - xce + \delta xce +  be 
  - bxe - \gamma be + \gamma xbe + 
  %%%
  c - c x - \gamma c + \gamma xc - \\ &
   ec + \gamma ce + x ce - \gamma xce} \\
  %%%%%%
  & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\cancel{\delta xbe} + \cancel{\delta xc} - \cancel{\delta xce} + \cancel{\gamma be} - \cancel{\gamma xbe} + \cancel{\gamma c} - 
  \cancel{\gamma xc} - \cancel{\gamma ce} + \cancel{\gamma xce} + \\ &
  \cancel{xbe} - \cancel{\delta xbe} + \cancel{cx} - \cancel{\delta xc} - \cancel{xce} + \cancel{\delta xce}  +  be 
  - \cancel{xbe} - \cancel{\gamma be} + \cancel{\gamma xbe}  + 
  c - \cancel{cx} - \cancel{\gamma c}  + \cancel{\gamma xc}  - \\ &
   ec + \cancel{\gamma ce} + \cancel{xce}  - \cancel{\gamma xce}} \\
   & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
   { be   +   c  -  ec} \\
   & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
   { e(b - c) + c}
\end{align*}


\begin{align*}
  & P(D|Y,\neg G) = \frac{P(D, Y, \neg G)}{P(Y, \neg G)} \\
  & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, D} P(Y, D, \neg G, T, E)} \\
  \scriptscriptstyle{
  & = \frac{
    \begin{aligned}
      & P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
  + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) 
  \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
\end{aligned}
 }{ 
  \begin{aligned} 
& P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
 \\ &
 +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
  + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
  \\ &
   +  P(\neg D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(\neg D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
 \\ &
  + P(\neg D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(\neg D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)}
\end{aligned}} \\
& = \frac{\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g)}
{\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g) + \\ &
(1- \delta ) xbf(1-g) + (1-\delta ) c(1-f)(1-g) + (1-\gamma ) (1-x)bf(1-g) + (1- \gamma) (1-x)c(1-f)(1-g)} \\
& = \frac{(1-g)(\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f))}
{(1-g)(\delta xbf + \delta xc(1-f)+ \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &(
1-  \delta) xbf + (1-\delta ) xc(1-f) + (1-\gamma ) (1-x)bf + (1-\gamma ) (1-x)c(1-f))} \\
& = \frac{\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f)}
{\delta xbf + \delta xc(1-f)+ \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &
(1-\delta) xbf + (1-\delta )xc(1-f) + (1-\gamma )(1-x)bf + (1-\gamma ) (1-x)c(1-f)} \\
%%%%%%%%
& = \frac{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
\gamma xc - \gamma cf + \gamma xcf
}{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
\gamma xc - \gamma cf + \gamma xcf + \\ &
%%%%sfcond half
xbf - \delta xbf + xc - \delta xc - xcf + \delta xcf +  bf 
- bxf - \gamma bf + \gamma xbf + 
%%%
c - c x - \gamma c + \gamma xc - \\ &
 fc + \gamma cf + x cf - \gamma xcf} \\
%%%%%%
& = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
{\cancel{\delta xbf} + \cancel{\delta xc} - \cancel{\delta xcf} + \cancel{\gamma bf} - \cancel{\gamma xbf} + \cancel{\gamma c} - 
\cancel{\gamma xc} - \cancel{\gamma cf} + \cancel{\gamma xcf} + \\ &
\cancel{xbf} - \cancel{\delta xbf} + \cancel{cx} - \cancel{\delta xc} - \cancel{xcf} + \cancel{\delta xcf}  +  bf 
- \cancel{xbf} - \cancel{\gamma bf} + \cancel{\gamma xbf}  + 
c - \cancel{cx} - \cancel{\gamma c}  + \cancel{\gamma xc}  - \\ &
 fc + \cancel{\gamma cf} + \cancel{xcf}  - \cancel{\gamma xcf}} \\
 & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
 { bf   +   c  -  fc} \\
 & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
 { f(b - c) + c}
\end{align*}





E is involved in the decision:

Error rate balance: 

\begin{align*}
  & P(D|Y,G) = \frac{P(D, Y, G)}{P(Y, G)} \\
  & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, D} P(Y, D, G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(\neg D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(\neg D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(\neg D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(\neg D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  } \\
  & = \frac{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e))
  }{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    & + (1 - \delta)x b e + (1 - \zeta)x c (1 - e) + (1 - \gamma)(1 - x) b e + (1 - \epsilon)(1 - x) c (1 - e))
  } \\
  & = \frac{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e)
  }{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    & + (1 - \delta)x b e + (1 - \zeta)x c (1 - e) + (1 - \gamma)(1 - x) b e + (1 - \epsilon)(1 - x) c (1 - e)} \\
%%%%
%%%%29122024
& = \frac{
    \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
    + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
  }{
    \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
    + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c \\
    & + x b e - \delta x b e  + x c - e x c - \zeta x c + e\zeta xc
    + b e - b e \gamma  -b e x + b e \gamma x  \\ 
    & +  c - x c - e c + x e c -
     c\epsilon + x c\epsilon + e c\epsilon - x e c\epsilon} \\
%%%
& = \frac{
    \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
    + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
  }{
    \cancel{\delta x b e} + \cancel{\zeta x c} - \cancel{e\zeta x c}  + \cancel{\gamma b e }- \cancel{\gamma x b e}
    + \cancel{ \epsilon c} - \cancel{e \epsilon c} - \cancel{ x c\epsilon } + \cancel{e x \epsilon c} \\
    & + \cancel{x b e} - \cancel{\delta x b e}  + \cancel{x c} -\cancel{ e x c} - \cancel{\zeta x c} + \cancel{e\zeta x c} 
    + b e 
    - \cancel{\gamma b e }  - \cancel{x b e}  + \cancel{\gamma x b e}  \\ 
    & +  
    c - \cancel{x c} 
    - e c 
    + \cancel{ e x c}  -
    \cancel{ \epsilon c} + \cancel{ x c\epsilon }+ \cancel{e \epsilon c} - \cancel{e x \epsilon c}} \\
    %%
    & = \frac{
      \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
      + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
    }{b e + c - e c} \\
   & = \frac{e(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
    }{e(b-c) - c} 
\end{align*}



E is involved, error rate balance, not-G

\begin{align*}
  & P(D|Y,\neg G) = \frac{P(D, Y, \neg G)}{P(Y, \neg G)} \\
  & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, D} P(Y, D, \neg G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(\neg D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(\neg D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(\neg D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(\neg D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  } \\
  & = \frac{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f))
  }{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    & + (1 - \delta)x b f + (1 - \zeta)x c (1 - f) + (1 - \gamma)(1 - x) b f + (1 - \epsilon)(1 - x) c (1 - f))
  } \\
  & = \frac{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f)
  }{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    & + (1 - \delta)x b f + (1 - \zeta)x c (1 - f) + (1 - \gamma)(1 - x) b f + (1 - \epsilon)(1 - x) c (1 - f)} \\
%%%%
%%%%29122024
& = \frac{
    \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
    + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
  }{
    \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
    + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c \\
    & + x b f - \delta x b f  + x c - f x c - \zeta x c + f\zeta xc
    + b f - b f \gamma  -b f x + b f \gamma x  \\ 
    & +  c - x c - f c + x f c -
     c\epsilon + x c\epsilon + f c\epsilon - x f c\epsilon} \\
%%%
& = \frac{
    \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
    + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
  }{
    \cancel{\delta x b f} + \cancel{\zeta x c} - \cancel{f\zeta x c}  + \cancel{\gamma b f }- \cancel{\gamma x b f}
    + \cancel{ \epsilon c} - \cancel{f \epsilon c} - \cancel{ x c\epsilon } + \cancel{f x \epsilon c} \\
    & + \cancel{x b f} - \cancel{\delta x b f}  + \cancel{x c} -\cancel{ f x c} - \cancel{\zeta x c} + \cancel{f\zeta x c} 
    + b f 
    - \cancel{\gamma b f }  - \cancel{x b f}  + \cancel{\gamma x b f}  \\ 
    & +  
    c - \cancel{x c} 
    - f c 
    + \cancel{ f x c}  -
    \cancel{ \epsilon c} + \cancel{ x c\epsilon }+ \cancel{f \epsilon c} - \cancel{f x \epsilon c}} \\
    %%
    & = \frac{
      \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
      + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
    }{b f + c - f c} \\
   & = \frac{f(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
    }{f(b-c) - c} 
\end{align*}


E is involved in the decision \\
PPV

\begin{align*}
  & P(Y|D,G) = \frac{P(Y, D, G)}{P(D, G)} \\
  & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, Y} P(Y, D, G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      %%%%SECOND HALF
      & + P(D|T, E)P(T|\neg Y)P(\neg Y|E)P(E|G)P(G) \\
      & + P( D|T, \neg E)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|\neg Y)P(\neg Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  } \\
  & = \frac{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e))
  }{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    %%%SPLIT
    & + \delta x (1-b) e + \zeta x (1-c) (1 - e) +  \gamma(1 - x) (1 -b) e +  \epsilon(1 - x) (1 -c) (1 - e))
  }  \\
  & = \frac{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e)
  }{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    %%%SPLIT
    & + \delta x (1-b) e + \zeta x (1-c) (1 - e) +  \gamma(1 - x) (1 -b) e +  \epsilon(1 - x) (1 -c) (1 - e)
  } \\
  %%%%
%%%%29122024
& = \frac{
  \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
  + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
}{
  \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
  + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c \\
  %%% second half
  & + 
  x \delta e - \delta x b e  + 
  x \zeta  - e x \zeta  - \zeta x c + e\zeta xc
  + \gamma e - \gamma e b  - \gamma e x + b e \gamma x  \\ 
  & +  \epsilon - x \epsilon - e \epsilon + x e \epsilon -
   c\epsilon + x c\epsilon + e c\epsilon - x e c\epsilon} \\
%%%
& = \frac{
  \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
  + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
}{
  \cancel{\delta x b e }+ \cancel{\zeta x c} - \cancel{e\zeta x c } + \cancel{\gamma b e} - \cancel{\gamma x b e}
  + \cancel{c\epsilon} - \cancel{e \epsilon c} - \cancel{x \epsilon c} + \cancel{e x \epsilon c} \\
  %%% second half
  & + 
  x \delta e 
  - \cancel{\delta x b e } + 
  x \zeta  - e x \zeta  - \cancel{\zeta x c} + \cancel{e\zeta x c }
  + \gamma e - \cancel{\gamma b e}   - \gamma e x + \cancel{\gamma x b e}  \\ 
  & +  \epsilon - x \epsilon - e \epsilon + x e \epsilon -
  \cancel{c\epsilon}  + \cancel{x \epsilon c}  + \cancel{e \epsilon c}  - \cancel{e x \epsilon c}} \\
%%
%%
& = \frac{
  e(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{
  x \delta e  + 
  x \zeta  - e x \zeta 
  + \gamma e - \gamma e x + \epsilon - x \epsilon - e \epsilon + x e \epsilon } \\
  %%%
  %%%
  %%%
  & = \frac{
  e(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{e( x(\delta - \zeta - \gamma + \epsilon) + \gamma  - \epsilon ) + x (\zeta - \epsilon ) + \epsilon} 
\end{align*}


Not-G

\begin{align*}
  & P(Y|D,\neg G) = \frac{P(Y, D, \neg G)}{P(D, \neg G)} \\
  & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, Y} P(Y, D, \neg G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      %%%%SECOND HALF
      & + P(D|T, E)P(T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) \\
      & + P( D|T, \neg E)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  } \\
  & = \frac{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f))
  }{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    %%%SPLIT
    & + \delta x (1-b) f + \zeta x (1-c) (1 - f) +  \gamma(1 - x) (1 -b) f +  \epsilon(1 - x) (1 -c) (1 - f))
  }  \\
  & = \frac{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f)
  }{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    %%%SPLIT
    & + \delta x (1-b) f + \zeta x (1-c) (1 - f) +  \gamma(1 - x) (1 -b) f +  \epsilon(1 - x) (1 -c) (1 - f)
  } \\
  %%%%
%%%%29122024
& = \frac{
  \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
  + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
}{
  \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
  + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c \\
  %%% sfcond half
  & + 
  x \delta f - \delta x b f  + 
  x \zeta  - f x \zeta  - \zeta x c + f\zeta xc
  + \gamma f - \gamma f b  - \gamma f x + b f \gamma x  \\ 
  & +  \epsilon - x \epsilon - f \epsilon + x f \epsilon -
   c\epsilon + x c\epsilon + f c\epsilon - x f c\epsilon} \\
%%%
& = \frac{
  \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
  + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
}{
  \cancel{\delta x b f }+ \cancel{\zeta x c} - \cancel{f\zeta x c } + \cancel{\gamma b f} - \cancel{\gamma x b f}
  + \cancel{c\epsilon} - \cancel{f \epsilon c} - \cancel{x \epsilon c} + \cancel{f x \epsilon c} \\
  %%% sfcond half
  & + 
  x \delta f 
  - \cancel{\delta x b f } + 
  x \zeta  - f x \zeta  - \cancel{\zeta x c} + \cancel{f\zeta x c }
  + \gamma f - \cancel{\gamma b f}   - \gamma f x + \cancel{\gamma x b f}  \\ 
  & +  \epsilon - x \epsilon - f \epsilon + x f \epsilon -
  \cancel{c\epsilon}  + \cancel{x \epsilon c}  + \cancel{f \epsilon c}  - \cancel{f x \epsilon c}} \\
%%
%%
& = \frac{
  f(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{
  x \delta f  + 
  x \zeta  - f x \zeta 
  + \gamma f - \gamma f x + \epsilon - x \epsilon - f \epsilon + x f \epsilon } \\
  %%%
  %%%
  %%%
  & = \frac{
  f(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{f( x(\delta - \zeta - \gamma + \epsilon) + \gamma  - \epsilon ) + x (\zeta - \epsilon ) + \epsilon} 
\end{align*}





%%%%APPENDIX 
%%%gotoappendix

\appendix
\appendixtitleon

\section{Appendix}
\subsection{Fair Models}

\subsubsection{Model A}

\textbf{Predictive parity, G:} \\

\begin{proof}
  \begin{align*}
    & P(Y|D,G) = \frac{P(Y, D, G)}{P(D, G)} \\
    & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, Y} P(Y, D, G, T, E)} \\
    \scriptscriptstyle{
    & = \frac{
      \begin{aligned}
        & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) 
    \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
  \end{aligned}
   }{ 
    \begin{aligned} 
  & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G)
   \\ &
   +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \\ &
     +  P(D|T)P(T|\neg Y)P(\neg Y|E)P(E|G)P(G) 
   + P(D|T)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G)
   \\ &
    + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|E)P(E|G)P(G) 
   + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G)}
  \end{aligned}} \\
  & = \frac{\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g}
  {\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g + \\ &
  \delta y(1-b)eg + \delta y(1-c)(1-e)g + \gamma (1-y)(1-b)eg + \gamma (1-y)(1-c)(1-e)g} \\
  & = \frac{g(\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e))}
  {g(\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &
  \delta y(1-b)e + \delta y(1-c)(1-e) + \gamma (1-y)(1-b)e + \gamma (1-y)(1-c)(1-e))} \\
  & = \frac{\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e)}
  {\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &
  \delta y(1-b)e + \delta y(1-c)(1-e) + \gamma (1-y)(1-b)e + \gamma (1-y)(1-c)(1-e)} \\
  & = \frac{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce
  }{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce + \\ &
  \delta ye - \delta ybe + \delta y - \delta yc -  \delta ye + \delta yce + \gamma e 
  - \gamma ye - \gamma be + \gamma ybe + \gamma - \gamma y - \gamma c + \gamma yc - \\ &
  \gamma e + \gamma ye + \gamma ce - \gamma yce} \\
  & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbe + \delta xc - \delta xce + \cancel{\gamma be} - \gamma xbe + \cancel{\gamma c}- 
  \gamma xc - \cancel{\gamma ce} + \gamma xce +
  \cancel{\delta ye} - \delta ybe + \\ &
   \delta y - \delta yc -  \cancel{\delta ye} + \delta yce + \cancel{\gamma e}
  - \cancel{\gamma ye} - \cancel{\gamma be} + \gamma ybe + \gamma - \gamma y - \cancel{\gamma c}- + \gamma yc  \\ &
  - \cancel{\gamma e} + \cancel{\gamma ye} + \cancel{\gamma ce} - \gamma yce} \\
  & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbe + \delta xc - \delta xce - \gamma xbe - 
  \gamma xc  + \gamma xce - \delta ybe + \\ &
   \delta y - \delta yc  + \delta yce +  + \gamma ybe + \gamma - \gamma y + \gamma yc  - \gamma yce} 
\end{align*}
%%the same proof continued
\begin{align*}
   & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
   {e(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
     + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma } \\
     & = \frac{e(xb(\delta - \gamma) - xc(\delta - \gamma)+ \gamma (b-c) ) + c(x(\delta - \gamma) + \gamma)}
     {e(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
       + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma }
\end{align*}
\end{proof}

\textbf{Predictive parity, $\neg $G:} \\
\begin{proof}
 \begin{align*}
 & P(Y|D,\neg G) = \frac{P(Y, D, \neg G)}{P(D, \neg G)} \\
    & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, Y} P(Y, D, \neg G, T, E)} \\
    \scriptscriptstylf{
    & = \frac{
      \begin{aligned}
        & P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
    + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) 
    \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
   + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
  \end{aligned}
   }{ 
    \begin{aligned} 
  & P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
   + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
   \\ &
   +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
    + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \\ &
     +  P(D|T)P(T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) 
   + P(D|T)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G)
   \\ &
    + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) 
   + P(D|\neg T)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G)}
  \end{aligned}} \\
  & = \frac{\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g)}
  {\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g) + \\ &
  \delta y(1-b)f(1-g) + \delta y(1-c)(1-f)(1-g) + \gamma (1-y)(1-b)f(1-g) + \gamma (1-y)(1-c)(1-f)(1-g)} \\
  & = \frac{(1-g)(\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f))}
  {(1-g)(\delta xbf+ \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &
  \delta y(1-b)f + \delta y(1-c)(1-f) + \gamma (1-y)(1-b)f+ \gamma (1-y)(1-c)(1-f))} \\
  & = \frac{\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f)}
  {\delta xbf+ \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &
  \delta y(1-b)f + \delta y(1-c)(1-f) + \gamma (1-y)(1-b)f+ \gamma (1-y)(1-c)(1-f)} \\
  & = \frac{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
  \gamma xc - \gamma cf + \gamma xcf
  }{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
  \gamma xc - \gamma cf + \gamma xcf + \\ &
  \delta yf - \delta ybf + \delta y - \delta yc -  \delta yf + \delta ycf + \gamma f 
  - \gamma yf - \gamma bf + \gamma ybf + \gamma - \gamma y - \gamma c + \gamma yc - \\ &
  \gamma f + \gamma yf + \gamma cf - \gamma ycf} \\
  & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbf + \delta xc - \delta xcf + \cancel{\gamma bf} - \gamma xbf + \cancel{\gamma c}- 
  \gamma xc - \cancel{\gamma cf} + \gamma xcf +
  \cancel{\delta yf} - \delta ybf + \\ &
   \delta y - \delta yc -  \cancel{\delta yf} + \delta ycf + \cancel{\gamma f}
  - \cancel{\gamma yf} - \cancel{\gamma bf} + \gamma ybf + \gamma - \gamma y - \cancel{\gamma c}- + \gamma yc  \\ &
  - \cancel{\gamma f} + \cancel{\gamma yf} + \cancel{\gamma cf} - \gamma ycf} \\
  & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\delta xbf + \delta xc - \delta xcf - \gamma xbf - 
  \gamma xc  + \gamma xcf - \delta ybf + \\ &
   \delta y - \delta yc  + \delta ycf +  + \gamma ybf + \gamma - \gamma y + \gamma yc  - \gamma ycf} 
 \end{align*}

 %%proof cont'd

 \begin{align*}
  & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {f(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
    + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma } \\
    & = \frac{f(xb(\delta - \gamma) - xc(\delta - \gamma)+ \gamma (b-c) ) + c(x(\delta - \gamma) + \gamma)}
  {f(xb(\delta - \gamma) - xc(\delta - \gamma) - yb(\delta - \gamma) + yc(\delta - \gamma))
    + xc(\delta - \gamma) - yc(\delta - \gamma) + y(\delta - \gamma) + \gamma }
 \end{align*}
\end{proof}


\textbf{Error rate balance, G}

\begin{proof}
  
  \begin{align*}
    & P(D|Y,G) = \frac{P(D, Y, G)}{P(Y, G)} \\
    & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, D} P(Y, D, G, T, E)} \\
    \scriptscriptstyle{
    & = \frac{
      \begin{aligned}
        & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) 
    \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
  \end{aligned}
   }{ 
    \begin{aligned} 
  & P(D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
   + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G)
   \\ &
   +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
    + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \\ &
     +  P(\neg D|T)P(T|Y)P(Y|E)P(E|G)P(G) 
   + P(\neg D|T)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G)
   \\ &
    + P(\neg D|\neg T)P(\neg T|Y)P(Y|E)P(E|G)P(G) 
   + P(\neg D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)}
  \end{aligned}} \\
  & = \frac{\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g}
  {\delta xbeg + \delta xc(1-e)g + \gamma (1-x)beg + \gamma (1-x)c(1-e)g + \\ &
(1-	\delta ) xbeg + (1-\delta ) c(1-e)g + (1-\gamma ) (1-x)beg + (1- \gamma) (1-x)c(1-e)g} \\
  & = \frac{g(\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e))}
  {g(\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &(
1-	\delta) xbe + (1-\delta ) xc(1-e) + (1-\gamma ) (1-x)be + (1-\gamma ) (1-x)c(1-e))} \\
  & = \frac{\delta xbe + \delta xc(1-e) + \gamma (1-x)be + \gamma (1-x)c(1-e)}
  {\delta xbe + \delta xc(1-e)+ \gamma (1-x)be + \gamma (1-x)c(1-e) + \\ &
  (1-\delta) xbe + (1-\delta )xc(1-e) + (1-\gamma )(1-x)be + (1-\gamma ) (1-x)c(1-e)} \\
  %%%%%%%%
  & = \frac{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce
  }{\delta xbe + \delta xc - \delta xce + \gamma be - \gamma xbe + \gamma c - 
  \gamma xc - \gamma ce + \gamma xce + \\ &
  %%%%second half
  xbe - \delta xbe + xc - \delta xc - xce + \delta xce +  be 
  - bxe - \gamma be + \gamma xbe + 
  %%%
  c - c x - \gamma c + \gamma xc - \\ &
   ec + \gamma ce + x ce - \gamma xce} \\
  %%%%%%
  & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
  {\cancel{\delta xbe} + \cancel{\delta xc} - \cancel{\delta xce} + \cancel{\gamma be} - \cancel{\gamma xbe} + \cancel{\gamma c} - 
  \cancel{\gamma xc} - \cancel{\gamma ce} + \cancel{\gamma xce} + \\ &
  \cancel{xbe} - \cancel{\delta xbe} + \cancel{cx} - \cancel{\delta xc} - \cancel{xce} + \cancel{\delta xce}  +  be 
  - \cancel{xbe} - \cancel{\gamma be} + \cancel{\gamma xbe}  + 
  c - \cancel{cx} - \cancel{\gamma c}  + \cancel{\gamma xc}  - \\ &
   ec + \cancel{\gamma ce} + \cancel{xce}  - \cancel{\gamma xce}} \\
   & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
   { be   +   c  -  ec} \\
   & = \frac{e(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
   { e(b - c) + c}
\end{align*}

\textbf{Error rate balance, $\neg$ G}

\begin{align*}
  & P(D|Y,\neg G) = \frac{P(D, Y, \neg G)}{P(Y, \neg G)} \\
  & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, D} P(Y, D, \neg G, T, E)} \\
  \scriptscriptstyle{
  & = \frac{
    \begin{aligned}
      & P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
  + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) 
  \\ & + P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
\end{aligned}
 }{ 
  \begin{aligned} 
& P(D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
 \\ &
 +  P(D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
  + P(D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
  \\ &
   +  P(\neg D|T)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(\neg D|T)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
 \\ &
  + P(\neg D|\neg T)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) 
 + P(\neg D|\neg T)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)}
\end{aligned}} \\
& = \frac{\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g)}
{\delta xbf(1-g) + \delta xc(1-f)(1-g) + \gamma (1-x)bf(1-g) + \gamma (1-x)c(1-f)(1-g) + \\ &
(1- \delta ) xbf(1-g) + (1-\delta ) c(1-f)(1-g) + (1-\gamma ) (1-x)bf(1-g) + (1- \gamma) (1-x)c(1-f)(1-g)} \\
& = \frac{(1-g)(\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f))}
{(1-g)(\delta xbf + \delta xc(1-f)+ \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &(
1-  \delta) xbf + (1-\delta ) xc(1-f) + (1-\gamma ) (1-x)bf + (1-\gamma ) (1-x)c(1-f))} \\
& = \frac{\delta xbf + \delta xc(1-f) + \gamma (1-x)bf + \gamma (1-x)c(1-f)}
{\delta xbf + \delta xc(1-f)+ \gamma (1-x)bf + \gamma (1-x)c(1-f) + \\ &
(1-\delta) xbf + (1-\delta )xc(1-f) + (1-\gamma )(1-x)bf + (1-\gamma ) (1-x)c(1-f)} \\
%%%%%%%%
& = \frac{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
\gamma xc - \gamma cf + \gamma xcf
}{\delta xbf + \delta xc - \delta xcf + \gamma bf - \gamma xbf + \gamma c - 
\gamma xc - \gamma cf + \gamma xcf + \\ &
%%%%sfcond half
xbf - \delta xbf + xc - \delta xc - xcf + \delta xcf +  bf 
- bxf - \gamma bf + \gamma xbf + 
%%%
c - c x - \gamma c + \gamma xc - \\ &
 fc + \gamma cf + x cf - \gamma xcf} \\
%%%%%%
& = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
{\cancel{\delta xbf} + \cancel{\delta xc} - \cancel{\delta xcf} + \cancel{\gamma bf} - \cancel{\gamma xbf} + \cancel{\gamma c} - 
\cancel{\gamma xc} - \cancel{\gamma cf} + \cancel{\gamma xcf} + \\ &
\cancel{xbf} - \cancel{\delta xbf} + \cancel{cx} - \cancel{\delta xc} - \cancel{xcf} + \cancel{\delta xcf}  +  bf 
- \cancel{xbf} - \cancel{\gamma bf} + \cancel{\gamma xbf}  + 
c - \cancel{cx} - \cancel{\gamma c}  + \cancel{\gamma xc}  - \\ &
 fc + \cancel{\gamma cf} + \cancel{xcf}  - \cancel{\gamma xcf}} \\
 & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
 { bf   +   c  -  fc} \\
 & = \frac{f(\delta xb - \delta xc + \gamma b - \gamma xb - \gamma c + \gamma xc) + c(\delta x + \gamma - \gamma x)}
 { f(b - c) + c}
\end{align*}

\subsubsection{Model B}

\textbf{Predictive Parity, G} \\

\begin{align*}
  & P(Y|D,G) = \frac{P(Y, D, G)}{P(D, G)} \\
  & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, Y} P(Y, D, G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      %%%%SECOND HALF
      & + P(D|T, E)P(T|\neg Y)P(\neg Y|E)P(E|G)P(G) \\
      & + P( D|T, \neg E)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|\neg Y)P(\neg Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  } \\
  & = \frac{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e))
  }{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    %%%SPLIT
    & + \delta x (1-b) e + \zeta x (1-c) (1 - e) +  \gamma(1 - x) (1 -b) e +  \epsilon(1 - x) (1 -c) (1 - e))
  }  \\
  & = \frac{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e)
  }{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    %%%SPLIT
    & + \delta x (1-b) e + \zeta x (1-c) (1 - e) +  \gamma(1 - x) (1 -b) e +  \epsilon(1 - x) (1 -c) (1 - e)
  } \\
  %%%%
%%%%29122024
& = \frac{
  \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
  + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
}{
  \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
  + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c \\
  %%% second half
  & + 
  x \delta e - \delta x b e  + 
  x \zeta  - e x \zeta  - \zeta x c + e\zeta xc
  + \gamma e - \gamma e b  - \gamma e x + b e \gamma x  \\ 
  & +  \epsilon - x \epsilon - e \epsilon + x e \epsilon -
   c\epsilon + x c\epsilon + e c\epsilon - x e c\epsilon} \\
%%%
& = \frac{
  \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
  + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
}{
  \cancel{\delta x b e }+ \cancel{\zeta x c} - \cancel{e\zeta x c } + \cancel{\gamma b e} - \cancel{\gamma x b e}
  + \cancel{c\epsilon} - \cancel{e \epsilon c} - \cancel{x \epsilon c} + \cancel{e x \epsilon c} \\
  %%% second half
  & + 
  x \delta e 
  - \cancel{\delta x b e } + 
  x \zeta  - e x \zeta  - \cancel{\zeta x c} + \cancel{e\zeta x c }
  + \gamma e - \cancel{\gamma b e}   - \gamma e x + \cancel{\gamma x b e}  \\ 
  & +  \epsilon - x \epsilon - e \epsilon + x e \epsilon -
  \cancel{c\epsilon}  + \cancel{x \epsilon c}  + \cancel{e \epsilon c}  - \cancel{e x \epsilon c}} \\
%%
%%
& = \frac{
  e(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{
  x \delta e  + 
  x \zeta  - e x \zeta 
  + \gamma e - \gamma e x + \epsilon - x \epsilon - e \epsilon + x e \epsilon } \\
  %%%
  %%%
  %%%
  & = \frac{
  e(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{e( x(\delta - \zeta - \gamma + \epsilon) + \gamma  - \epsilon ) + x (\zeta - \epsilon ) + \epsilon} 
\end{align*}


\textbf{Predictive Parity, $\neg$G}

\begin{align*}
  & P(Y|D,\neg G) = \frac{P(Y, D, \neg G)}{P(D, \neg G)} \\
  & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, Y} P(Y, D, \neg G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      %%%%SECOND HALF
      & + P(D|T, E)P(T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) \\
      & + P( D|T, \neg E)P(T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|\neg Y)P(\neg Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|\neg Y)P(\neg Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  } \\
  & = \frac{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f))
  }{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    %%%SPLIT
    & + \delta x (1-b) f + \zeta x (1-c) (1 - f) +  \gamma(1 - x) (1 -b) f +  \epsilon(1 - x) (1 -c) (1 - f))
  }  \\
  & = \frac{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f)
  }{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    %%%SPLIT
    & + \delta x (1-b) f + \zeta x (1-c) (1 - f) +  \gamma(1 - x) (1 -b) f +  \epsilon(1 - x) (1 -c) (1 - f)
  } \\
  %%%%
%%%%29122024
& = \frac{
  \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
  + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
}{
  \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
  + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c \\
  %%% sfcond half
  & + 
  x \delta f - \delta x b f  + 
  x \zeta  - f x \zeta  - \zeta x c + f\zeta xc
  + \gamma f - \gamma f b  - \gamma f x + b f \gamma x  \\ 
  & +  \epsilon - x \epsilon - f \epsilon + x f \epsilon -
   c\epsilon + x c\epsilon + f c\epsilon - x f c\epsilon} \\
%%%
& = \frac{
  \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
  + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
}{
  \cancel{\delta x b f }+ \cancel{\zeta x c} - \cancel{f\zeta x c } + \cancel{\gamma b f} - \cancel{\gamma x b f}
  + \cancel{c\epsilon} - \cancel{f \epsilon c} - \cancel{x \epsilon c} + \cancel{f x \epsilon c} \\
  %%% sfcond half
  & + 
  x \delta f 
  - \cancel{\delta x b f } + 
  x \zeta  - f x \zeta  - \cancel{\zeta x c} + \cancel{f\zeta x c }
  + \gamma f - \cancel{\gamma b f}   - \gamma f x + \cancel{\gamma x b f}  \\ 
  & +  \epsilon - x \epsilon - f \epsilon + x f \epsilon -
  \cancel{c\epsilon}  + \cancel{x \epsilon c}  + \cancel{f \epsilon c}  - \cancel{f x \epsilon c}} \\
%%
%%
& = \frac{
  f(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{
  x \delta f  + 
  x \zeta  - f x \zeta 
  + \gamma f - \gamma f x + \epsilon - x \epsilon - f \epsilon + x f \epsilon } \\
  %%%
  %%%
  %%%
  & = \frac{
  f(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
}{f( x(\delta - \zeta - \gamma + \epsilon) + \gamma  - \epsilon ) + x (\zeta - \epsilon ) + \epsilon} 
\end{align*}

\textbf{Error rate balance, G}

\begin{align*}
  & P(D|Y,G) = \frac{P(D, Y, G)}{P(Y, G)} \\
  & = \frac{\sum_{T,E} P(Y, D, G, T, E) }{\sum_{T,E, D} P(Y, D, G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(\neg D|T, E)P(T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(\neg D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|G)P(G) \\
      & + P(\neg D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|G)P(G) \\
      & + P(\neg D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|G)P(G)
    \end{aligned}
  } \\
  & = \frac{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e))
  }{
    g(\delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    & + (1 - \delta)x b e + (1 - \zeta)x c (1 - e) + (1 - \gamma)(1 - x) b e + (1 - \epsilon)(1 - x) c (1 - e))
  } \\
  & = \frac{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e)
  }{
    \delta x b e + \zeta x c (1 - e) + \gamma (1 - x) b e + \epsilon (1 - x) c (1 - e) \\
    & + (1 - \delta)x b e + (1 - \zeta)x c (1 - e) + (1 - \gamma)(1 - x) b e + (1 - \epsilon)(1 - x) c (1 - e)} \\
%%%%
%%%%29122024
& = \frac{
    \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
    + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
  }{
    \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
    + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c \\
    & + x b e - \delta x b e  + x c - e x c - \zeta x c + e\zeta xc
    + b e - b e \gamma  -b e x + b e \gamma x  \\ 
    & +  c - x c - e c + x e c -
     c\epsilon + x c\epsilon + e c\epsilon - x e c\epsilon} \\
%%%
& = \frac{
    \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
    + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
  }{
    \cancel{\delta x b e} + \cancel{\zeta x c} - \cancel{e\zeta x c}  + \cancel{\gamma b e }- \cancel{\gamma x b e}
    + \cancel{ \epsilon c} - \cancel{e \epsilon c} - \cancel{ x c\epsilon } + \cancel{e x \epsilon c} \\
    & + \cancel{x b e} - \cancel{\delta x b e}  + \cancel{x c} -\cancel{ e x c} - \cancel{\zeta x c} + \cancel{e\zeta x c} 
    + b e 
    - \cancel{\gamma b e }  - \cancel{x b e}  + \cancel{\gamma x b e}  \\ 
    & +  
    c - \cancel{x c} 
    - e c 
    + \cancel{ e x c}  -
    \cancel{ \epsilon c} + \cancel{ x c\epsilon }+ \cancel{e \epsilon c} - \cancel{e x \epsilon c}} \\
    %%
    & = \frac{
      \delta x b e + \zeta x c - e\zeta x c  + \gamma b e - \gamma x b e
      + \epsilon c -e \epsilon c -x \epsilon c + e x \epsilon c
    }{b e + c - e c} \\
   & = \frac{e(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
    }{e(b-c) - c} 
\end{align*}



\textbf{Error rate balance, $\neg$G}

\begin{align*}
  & P(D|Y,\neg G) = \frac{P(D, Y, \neg G)}{P(Y, \neg G)} \\
  & = \frac{\sum_{T,E} P(Y, D, \neg G, T, E) }{\sum_{T,E, D} P(Y, D, \neg G, T, E)} \\
  & = \frac{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  }{
    \begin{aligned}
      & P(D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(\neg D|T, E)P(T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(\neg D|T, \neg E)P(T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G) \\
      & + P(\neg D|\neg T, E)P(\neg T|Y)P(Y|E)P(E|\neg G)P(\neg G) \\
      & + P(\neg D|\neg T, \neg E)P(\neg T|Y)P(Y|\neg E)P( \neg E|\neg G)P(\neg G)
    \end{aligned}
  } \\
  & = \frac{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f))
  }{
    (1-g)(\delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    & + (1 - \delta)x b f + (1 - \zeta)x c (1 - f) + (1 - \gamma)(1 - x) b f + (1 - \epsilon)(1 - x) c (1 - f))
  } \\
  & = \frac{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f)
  }{
    \delta x b f + \zeta x c (1 - f) + \gamma (1 - x) b f + \epsilon (1 - x) c (1 - f) \\
    & + (1 - \delta)x b f + (1 - \zeta)x c (1 - f) + (1 - \gamma)(1 - x) b f + (1 - \epsilon)(1 - x) c (1 - f)} \\
%%%%
%%%%29122024
& = \frac{
    \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
    + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
  }{
    \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
    + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c \\
    & + x b f - \delta x b f  + x c - f x c - \zeta x c + f\zeta xc
    + b f - b f \gamma  -b f x + b f \gamma x  \\ 
    & +  c - x c - f c + x f c -
     c\epsilon + x c\epsilon + f c\epsilon - x f c\epsilon} \\
%%%
& = \frac{
    \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
    + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
  }{
    \cancel{\delta x b f} + \cancel{\zeta x c} - \cancel{f\zeta x c}  + \cancel{\gamma b f }- \cancel{\gamma x b f}
    + \cancel{ \epsilon c} - \cancel{f \epsilon c} - \cancel{ x c\epsilon } + \cancel{f x \epsilon c} \\
    & + \cancel{x b f} - \cancel{\delta x b f}  + \cancel{x c} -\cancel{ f x c} - \cancel{\zeta x c} + \cancel{f\zeta x c} 
    + b f 
    - \cancel{\gamma b f }  - \cancel{x b f}  + \cancel{\gamma x b f}  \\ 
    & +  
    c - \cancel{x c} 
    - f c 
    + \cancel{ f x c}  -
    \cancel{ \epsilon c} + \cancel{ x c\epsilon }+ \cancel{f \epsilon c} - \cancel{f x \epsilon c}} \\
    %%
    & = \frac{
      \delta x b f + \zeta x c - f\zeta x c  + \gamma b f - \gamma x b f
      + \epsilon c -f \epsilon c -x \epsilon c + f x \epsilon c
    }{b f + c - f c} \\
   & = \frac{f(\delta x b  -\zeta x c + \gamma b - \gamma x b - \epsilon c  + x \epsilon c ) + c(x(\zeta - \epsilon) + \epsilon)
    }{f(b-c) - c} 
\end{align*}

\subsubsection{Model C}

\textbf{Predictive Parity, G}

\begin{align*}
  & P(Y|D,G) = \frac{P(Y, D, G)}{P(D, G)} \\
  & = \frac{\sum_{E} P(Y, D, G, T, E) }{\sum_{E, Y} P(Y, D, G, E)} \\
  \scriptscriptstyle{
  & = \frac{
    \begin{aligned}
      & P(D|E)P(Y|E)P(E|G)P(G) 
  + P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
\end{aligned}
 }{ 
  \begin{aligned} 
& P(D|E)P(Y|E)P(E|G)P(G) 
+ P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
  \\ &
   +  P(D|E)P(\neg Y|E)P(E|G)P(G) 
 + P(D|\neg E)P(\neg Y|\neg E)P( \neg E|G)P(G)}
\end{aligned}} \\
& = \frac{\delta beg + \epsion c(1-e)g}{\delta beg + \epsion c(1-e)g \\ &
+ \delta (1-b) e g + \epsilon (1-c) (1-e) g
}\\
& = \frac{g(\delta be + \epsion c(1-e))}{g(\delta be + \epsion c(1-e) \\ &
+ \delta (1-b) e  + \epsilon (1-c) (1-e)) 
}\\
& = \frac{e(\delta b - \epsilon c) +  \epsilon c
}{e(\delta b - \epsilon c) +  \epsilon  
}
\end{align*}



\textbf{Predictive Parity, $\neg$ G}




Error rate balance, model c

\begin{align*}
  & P(D|Y,G) = \frac{P(Y, D, G)}{P(Y, G)} \\
  & = \frac{\sum_{E} P(Y, D, G, T, E) }{\sum_{E, D} P(Y, D, G, E)} \\
  \scriptscriptstyle{
  & = \frac{
    \begin{aligned}
      & P(D|E)P(Y|E)P(E|G)P(G) 
  + P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
\end{aligned}
 }{ 
  \begin{aligned} 
& P(D|E)P(Y|E)P(E|G)P(G) 
+ P(D|\neg E)P(Y|\neg E)P( \neg E|G)P(G) 
  \\ &
   +  P(\neg D|E)P(Y|E)P(E|G)P(G) 
 + P(\neg D|\neg E)P(Y|\neg E)P( \neg E|G)P(G)}
\end{aligned}} \\
& = \frac{\delta beg + \epsion c(1-e)g}{\delta beg + \epsion c(1-e)g \\ &
+ (1 - \delta) b e g + (1- \epsilon) c (1-e) g
}\\
& = \frac{g(\delta be + \epsion c(1-e))}{g(\delta be + \epsion c(1-e) \\ &
+ (1 - \delta) b e + (1- \epsilon) c (1-e))
}\\
& = \frac{e(\delta b - \epsilon c) +  \epsilon c
}{e(b - c) +  c 
}
\end{align*}








%%%END APPENDIX




\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}
\bibliographystyle{apacite}
\bibliography{af}

\end{document}



